\documentclass[a4paper,11pt]{article}
\usepackage[left=1.5cm,right=1.5cm,top=2.0cm,bottom=2.8cm]{geometry}

\usepackage[ngerman,american]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{epstopdf}
\usepackage{sidecap}
% \usepackage[FIGTOPCAP]{subfigure}
\usepackage{float}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage{empheq}
\usepackage[footnotesize,bf]{caption}
\usepackage{subcaption}
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

\theoremstyle{definition}
\newtheorem{defi}{Definition}
\theoremstyle{plain}
\newtheorem{theo}[defi]{Theorem}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\title{Exercise 6}
\author{Philipp Hanslovsky, Robert Walecki}

\begin{document}

\lstset{language=Matlab,
   keywords={break,case,catch,continue,else,elseif,end,for,function,
      global,if,otherwise,persistent,return,switch,try,while},
   basicstyle=\ttfamily,
   keywordstyle=\color{blue},
   commentstyle=\color{green!40!black},
   stringstyle=\color{red},
   numbers=left,
   numberstyle=\tiny\color{white!50!black},
   stepnumber=1,
   numbersep=10pt,
   backgroundcolor=\color{white},
   tabsize=4,
   showspaces=false,
   showstringspaces=false,
   frame=single}


\maketitle

\section*{6.2.2}

The gini $\Delta$ as well as class distributions for labels 1 and 2 are shown in figure \ref{fig:gini}. The y-value of the corresponding label does not have a meaning. It just makes both labels more distinct. The x-value of the largest gini $\Delta$ is where both classes have the least overlap.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{gidelta.pdf}
\caption{gini $\Delta$ as a function of the split value. The class distributions are shown for label 1 (blue) and label 2 (green).}
\label{fig:gini}
\end{figure}

\section*{6.3.1}
The covariance is a symmetric bilinear form. Therefore the following rules apply:
\begin{align}
Cov(X,Y) =& Cov(Y,X) \label{eq:bf_1} \\
Cov(aX+b,Y) =& aCov(X,Y) \label{eq:bf_2} \\
Cov(X+Y,Z) =& Cov(X,Z)+Cov(Y,Z) \label{eq:bf_3}
\end{align}
\ref{eq:bf_3} can be generalized to:
\begin{align}
Cov(\sum_iX_i, Y) =& \sum_iCov(X_i,Y) \label{eq:bf_4}
\end{align}
Furthermore the covariance is a generalization of the variance:
\begin{align}
Var(X) =& Cov(X,X) \label{eq:cov}
\end{align}
The relation between covariance and correlation is given by:
\begin{align}
\rho_{ij} =& \frac{cov(X_i,X_j}{\sigma_i\sigma_j} \overset{i.d.}{=} \frac{cov(X_i,X_j)}{\sigma^2} \label{eq:cor}
\end{align}
With this preconditions we get:
\begin{align}
Var(\frac{1}{B}\sum_{i=1}^BX_i) \overset{\ref{eq:cov}}{=}& Cov(\frac{1}{B}\sum_{i=1}^BX_i,\frac{1}{B}\sum_{i=1}^BX_i) \\
  \overset{\ref{eq:bf_2}, \ref{eq:bf_3}}{=}&\frac{1}{B^2}\sum_{i,j=1}^B(X_i,X_j) \\
  \overset{\ref{eq:cov}}{=}&\frac{1}{B^2}sum_{i=1}^BVar(X_i)+\frac{2}{B^2}\sum_{i=1}^{B-1}\sum_{j=i+1}^Bcov(X_i,X_j) \\
  \overset{\ref{eq:cor}, i.d.}{=}&\frac{1}{B^2}B\sigma^2+\frac{2\rho\sigma^2}{B^2}\sum_{i=1}^{B-1}\sum_{j=i+1}^B1 \\
  =&\frac{\sigma^2}{B}+\frac{2\rho\sigma^2}{B^2}\sum_{i=1}^{B-1}B-i \\
  =&\frac{\sigma^2}{B}+\frac{2\rho\sigma^2}{B^2}\left(B\left(B-1\right) - \frac{B\left(B-1\right)}{2}\right) \\
  =&\frac{\sigma^2}{B}+\frac{\rho\sigma^2}{B}(B-1) \\
  =&\rho\sigma^2+\frac{(1-\rho)\sigma^2}{B}
\end{align} 

\section*{6.3.2}
The probability $p_{oob}$ for an observation to be out of bag is given by the ratio of the number $N_{all}$ of all possible bags and the number $N_{-1}$ of all possible bags that do not contain given observation. Let $N$ be the number of all observations and $k$ the number of elements contained in a bag. Then the total number of possible bags is given by the number of possible configurations $\frac{N!}{(N-k)!}$ divided by the number of corresponding permutations $k!$ (i.e. choosing observations 1, 2 and 4 is the same as choosing 1, 4 and 2) or the binomial coefficient.
\begin{align}
N_{all} &= \frac{N!}{k!(N-k)!} \\
&= \binom{N}{k}
\end{align}
The number of possible bags not containing a certain observation can be obtained in a similar fashion. Let $x_i$ be an observation fixed to be not in a bag. Then the number of possible bags for that configuration is given by the number of possible configurations for the remaining $N-1$ samples $\frac{(N-1)!}{(N-k-1)!}$ divided by the number of corresponding permutations $k!$.
\begin{align}
N_{-1} &= \frac{(N-1)!}{k!(N-1-k)!} \\
&= \binom{N-1}{k}
\end{align}
Therefore the resulting probability $p_{oob}$ is given by:
\begin{align}
p_{oob} &= \frac{N_{-1}}{N_{all}} \\
&=\frac{(N-1)!}{k!(N-1-k)!}\frac{k!(N-k)!}{N!} \\
p_{oob} &= \frac{N-k}{N} = 1 - \frac{k}{N}
\end{align}
That probability holds for a single bootstrap sample. Given M trees based on M bootstrap samples, the probability for an observation to be out-of-bag in at least one tree can be calculated with the help of the probability that the observation is contained in every bootstrap sample. $(1-p_{oob})^M$ is the probability for an observation to be contained in all bootstrap samples.
\begin{align}
p &= 1-(1-p_{oob})^M \\
p &=1-(\frac{k}{N})^M \label{eq:oob}
\end{align}

\clearpage
\section*{6.3.3}

The oob error decreases with an increasing number of trees M. The test error is decreasing as well, however it's fluctuating, so it might be constant for $M > M_{min}$. Therefore choosing a good k might save a lot of computation time.
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\textwidth]{oob.pdf}
\caption{oob error}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\textwidth]{test_error.pdf}
\caption{test error}
\end{subfigure}
\caption{oob and test error as functions of the number of trees}
\end{figure}
\end{document}
